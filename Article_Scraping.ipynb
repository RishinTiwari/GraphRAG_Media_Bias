{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caab1d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▌                                  | 12/60 [00:14<00:37,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to access https://www.nationalreview.com/news/trump-zelensky-go-at-it-in-heated-oval-office-debate-youre-gambling-with-world-war-iii/: Status code 403\n",
      "Failed to access https://www.wsj.com/world/zelensky-to-meet-trump-in-bid-to-salvage-u-s-support-2e656025: Status code 401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|████████████████▍                          | 23/60 [00:30<00:47,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to access https://thehill.com/newsletters/business-economy/5256455-trumps-tariff-problem/: Status code 403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████████████████████                       | 28/60 [00:36<00:42,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape https://www.newsmax.com/us/trade-war-tariffs/2025/04/23/id/1208015/: HTTPSConnectionPool(host='www.newsmax.com', port=443): Read timed out. (read timeout=10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|███████████████████████████▉               | 39/60 [01:02<00:36,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to access https://www.reuters.com/sports/basketball/florida-beat-houston-claim-third-ncaa-mens-basketball-title-2025-04-08/: Status code 401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|█████████████████████████████▍             | 41/60 [01:04<00:23,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to access https://www.washingtontimes.com/news/2025/apr/7/florida-wins-ncaa-basketball-title-rallying-beat-houston-65-63/: Status code 403\n",
      "Failed to scrape nan: Invalid URL 'nan': No scheme supplied. Perhaps you meant https://nan?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [01:36<00:00,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraping complete and file saved as 'updated_table_with_content.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from newspaper import Article\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Step 1: Load table\n",
    "df = pd.read_csv('dataset.csv')  # or use read_excel\n",
    "\n",
    "# Headers to mimic a real browser\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                  \"Chrome/122.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Step 2: Scrape articles\n",
    "df['Content'] = ''\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    url = row['Link']\n",
    "    try:\n",
    "        # Make request manually with headers\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to access {url}: Status code {response.status_code}\")\n",
    "            df.at[idx, 'Content'] = ''\n",
    "            continue\n",
    "\n",
    "        # Pass HTML manually to newspaper\n",
    "        article = Article(url)\n",
    "        article.set_html(response.text)\n",
    "        article.parse()\n",
    "\n",
    "        df.at[idx, 'Content'] = article.text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        df.at[idx, 'Content'] = ''\n",
    "    \n",
    "    time.sleep(1)  # polite delay between requests\n",
    "\n",
    "# Step 3: Save table\n",
    "df.to_csv('updated_table_with_content.csv', index=False)\n",
    "print(\"✅ Scraping complete and file saved as 'updated_table_with_content.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568ff0b1",
   "metadata": {},
   "source": [
    "Step | Description\n",
    "1. | Loads your scraped articles\n",
    "2. | Extracts all PERSON entities (names)\n",
    "3. | Groups similar names automatically\n",
    "4. | Builds a normalization dictionary\n",
    "5. | Replaces variations in articles with the canonical names\n",
    "6. | Tracks everything nicely with tqdm progress bars\n",
    "7. | Saves the new dataframe with a Content_Normalized column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db006b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data and model...\n",
      "Extracting named entities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 52/52 [00:07<00:00,  7.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total PERSON entities extracted: 966\n",
      "PERSON entities after cleaning: 957\n",
      "Normalization dictionary size: 66\n",
      "\n",
      "Sample normalization mappings:\n",
      "  Donald Trump  -->  Trump\n",
      "  Eric Trump  -->  Trump\n",
      "  JD Vance  -->  Vance\n",
      "  J. D. Vance  -->  Vance\n",
      "  VP Vance  -->  Vance\n",
      "  Vladimir Putin  -->  Putin\n",
      "  Volodymyr Zelenskyy  -->  Zelenskyy\n",
      "  Hunter Biden  -->  Joe Biden\n",
      "  Biden  -->  Joe Biden\n",
      "  Clinton  -->  Hillary Clinton\n",
      "\n",
      "Normalizing articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 60/60 [00:00<00:00, 293.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final normalized articles saved as 'final_normalized_articles.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Load Data and spaCy model\n",
    "print(\"Loading data and model...\")\n",
    "df = pd.read_csv('updated_table_with_content.csv')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Step 1: Extract Named Entities (PERSON)\n",
    "def extract_named_entities(texts):\n",
    "    persons = []\n",
    "    print(\"Extracting named entities...\")\n",
    "    for doc in tqdm(nlp.pipe(texts, batch_size=20), total=len(texts)):\n",
    "        persons.extend([ent.text.strip() for ent in doc.ents if ent.label_ == \"PERSON\"])\n",
    "    return persons\n",
    "\n",
    "all_persons = extract_named_entities(df['Content'].dropna().tolist())\n",
    "print(f\"Total PERSON entities extracted: {len(all_persons)}\")\n",
    "\n",
    "# Step 2: Clean and Filter NEs\n",
    "def clean_persons(persons):\n",
    "    cleaned = []\n",
    "    for p in persons:\n",
    "        if len(p.split()) <= 3 and all(word[0].isupper() for word in p.split() if word.isalpha()):\n",
    "            cleaned.append(p)\n",
    "    return cleaned\n",
    "\n",
    "filtered_persons = clean_persons(all_persons)\n",
    "print(f\"PERSON entities after cleaning: {len(filtered_persons)}\")\n",
    "\n",
    "# Step 3: Build Normalization Dictionary (Smarter Grouping)\n",
    "def build_normalization_dict(person_list):\n",
    "    groups = defaultdict(list)\n",
    "    \n",
    "    for person in person_list:\n",
    "        parts = person.replace(\".\", \"\").split()\n",
    "        if parts:\n",
    "            last_name = parts[-1].lower()\n",
    "            groups[last_name].append(person)\n",
    "    \n",
    "    normalization_dict = {}\n",
    "    for group, names in groups.items():\n",
    "        name_counter = Counter(names)\n",
    "        canonical_name = max(name_counter, key=name_counter.get)  # most frequent\n",
    "        for name in names:\n",
    "            if name != canonical_name:\n",
    "                normalization_dict[name] = canonical_name\n",
    "    \n",
    "    return normalization_dict\n",
    "\n",
    "normalization_dict = build_normalization_dict(filtered_persons)\n",
    "print(f\"Normalization dictionary size: {len(normalization_dict)}\")\n",
    "\n",
    "# Optional: See top mapped examples\n",
    "print(\"\\nSample normalization mappings:\")\n",
    "for i, (k, v) in enumerate(normalization_dict.items()):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    print(f\"  {k}  -->  {v}\")\n",
    "\n",
    "# Step 4: Normalize Content\n",
    "def normalize_content(text, normalization_dict):\n",
    "    if pd.isna(text) or text.strip() == '':\n",
    "        return text\n",
    "    for pattern, replacement in normalization_dict.items():\n",
    "        text = re.sub(rf'\\b{re.escape(pattern)}\\b', replacement, text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "print(\"\\nNormalizing articles...\")\n",
    "tqdm.pandas()\n",
    "df['Content_Normalized'] = df['Content'].progress_apply(lambda x: normalize_content(x, normalization_dict))\n",
    "\n",
    "# Step 5: Save the final file\n",
    "df.to_csv('final_normalized_articles.csv', index=False)\n",
    "print(\"\\n✅ Final normalized articles saved as 'final_normalized_articles.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f3d163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d67b66e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "362f1c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting entities and relations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 52/52 [00:08<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique PERSON nodes: 324\n",
      "Total unique relationships: 198\n",
      "Creating nodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 324/324 [00:45<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating relationships...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 198/198 [00:26<00:00,  7.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Graph populated successfully into your Neo4j Aura database!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from neo4j import GraphDatabase\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Neo4j Connection\n",
    "URI = \"neo4j+s://da0d5023.databases.neo4j.io\"\n",
    "AUTH = (\"neo4j\", \"\")\n",
    "driver = GraphDatabase.driver(URI, auth=AUTH)\n",
    "\n",
    "# 2. Load Data and spaCy Model\n",
    "df = pd.read_csv('final_normalized_articles.csv')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 3. Extract Entities and Relationships\n",
    "def extract_entities_relations(text):\n",
    "    doc = nlp(text)\n",
    "    persons = [ent.text.strip() for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "    relations = []\n",
    "    for sent in doc.sents:\n",
    "        persons_in_sent = [ent.text.strip() for ent in sent.ents if ent.label_ == \"PERSON\"]\n",
    "        if len(persons_in_sent) >= 2:\n",
    "            for i in range(len(persons_in_sent) - 1):\n",
    "                relations.append((persons_in_sent[i], \"RELATED_TO\", persons_in_sent[i+1]))\n",
    "    return set(persons), set(relations)\n",
    "\n",
    "all_nodes = set()\n",
    "all_edges = set()\n",
    "\n",
    "print(\"Extracting entities and relations...\")\n",
    "for text in tqdm(df['Content_Normalized'].dropna().tolist()):\n",
    "    nodes, edges = extract_entities_relations(text)\n",
    "    all_nodes.update(nodes)\n",
    "    all_edges.update(edges)\n",
    "\n",
    "print(f\"Total unique PERSON nodes: {len(all_nodes)}\")\n",
    "print(f\"Total unique relationships: {len(all_edges)}\")\n",
    "\n",
    "# 4. Helper function to run Cypher safely\n",
    "def run_query(tx, query, parameters=None):\n",
    "    tx.run(query, parameters or {})\n",
    "\n",
    "# 5. Insert into Neo4j\n",
    "with driver.session() as session:\n",
    "    print(\"Creating nodes...\")\n",
    "    for person in tqdm(all_nodes):\n",
    "        query = \"\"\"\n",
    "        MERGE (p:Person {name: $name})\n",
    "        \"\"\"\n",
    "        session.execute_write(run_query, query, {\"name\": person})\n",
    "\n",
    "    print(\"Creating relationships...\")\n",
    "    for source, relation, target in tqdm(all_edges):\n",
    "        query = \"\"\"\n",
    "        MATCH (a:Person {name: $source})\n",
    "        MATCH (b:Person {name: $target})\n",
    "        MERGE (a)-[r:RELATED_TO]->(b)\n",
    "        \"\"\"\n",
    "        session.execute_write(run_query, query, {\"source\": source, \"target\": target})\n",
    "\n",
    "print(\"✅ Graph populated successfully into your Neo4j Aura database!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a64cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
